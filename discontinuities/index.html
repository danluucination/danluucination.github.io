<!DOCTYPE html><html lang=en><meta charset=utf-8><title>Suspicious discontinuities</title><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><link rel="stylesheet" href="/styles.css"></style><link rel=icon href="data:;base64,="> <header><strong>Suspicious discontinuities</strong> | <a href=https://patreon.com/danluu>Patreon</a><hr></header><main> <p>If you read any personal finance forums late last year, there's a decent chance you ran across a question from someone who was desperately trying to lose money before the end of the year. There are a number of ways someone could do this; one commonly suggested scheme was to buy <a href=https://en.wikipedia.org/wiki/Put_option>put options that were expected to expire worthless</a>, allowing the buyer to (probably) take a loss.</p> <p>One reason people were looking for ways to lose money was that, in the U.S., there's <a href=https://en.wikipedia.org/wiki/Patient_Protection_and_Affordable_Care_Act#Subsidy_Cliff_at_400%_FPL>a hard income cutoff for a health insurance subsidy</a> at $48,560 for individuals (higher for larger households; $100,400 for a family of four). There are a number of factors that can cause the details to vary (age, location, household size, type of plan), but across all circumstances, it wouldn't have been uncommon for an individual going from one side of the cut-off to the other to have their health insurance cost increase by roughly $7200/yr. That means if an individual buying ACA insurance was going to earn $55k, they'd be better off reducing their income by $6440 and getting under the $48,560 subsidy ceiling than they are earning $55k.</p> <p>Although that's an unusually severe example, <a href=http://www.cbo.gov/sites/default/files/cbofiles/attachments/11-15-2012-MarginalTaxRates.pdf>U.S. tax policy is full of discontinuities that disincentivize increasing earnings and, in some cases, actually incentivize decreasing earnings</a>. Some other discontinuities are the <a href=https://en.wikipedia.org/wiki/Temporary_Assistance_for_Needy_Families>TANF</a> income limit, the <a href=https://en.wikipedia.org/wiki/Medicaid>Medicaid</a> income limit, the <a href=https://en.wikipedia.org/wiki/Children%27s_Health_Insurance_Program>CHIP</a> income limit for free coverage, and the CHIP income limit for reduced-cost coverage. These vary by location and circumstance; the TANF and Medicaid income limits fall into ranges generally considered to be &quot;low income&quot; and the CHIP limits fall into ranges generally considered to be &quot;middle class&quot;. These subsidy discontinuities have the same impact as the ACA subsidy discontinuity -- at certain income levels, people are incentivized to lose money.</p> <blockquote> <p>Anyone may arrange his affairs so that his taxes shall be as low as possible; he is not bound to choose that pattern which best pays the treasury. There is not even a patriotic duty to increase one's taxes. Over and over again the Courts have said that there is nothing sinister in so arranging affairs as to keep taxes as low as possible. Everyone does it, rich and poor alike and all do right, for nobody owes any public duty to pay more than the law demands.</p> </blockquote> <p>If you agree with the famous <a href=https://en.wikipedia.org/wiki/Learned_Hand>Learned Hand</a> quote then losing money in order to reduce effective tax rate, increasing disposable income, is completely legitimate behavior at the individual level. However, a tax system that encourages people to lose money, perhaps by funneling it to (on average) much wealthier options traders by buying put options, seems sub-optimal.</p> <p>A simple fix for the problems mentioned above would be to have slow phase-outs instead of sharp thresholds. Slow phase-outs are actually done for some subsidies and, while that can also have problems, they are typically less problematic than introducing a sharp discontinuity in tax/subsidy policy.</p> <p>In this post, we'll look at a variety of discontinuities.</p> <h3 id=hardware-or-software-queues>Hardware or software queues</h3> <p>A naive queue has discontinuous behavior. If the queue is full, new entries are dropped. If the queue isn't full, new entries are not dropped. Depending on your goals, this can often have impacts that are non-ideal. For example, in networking, a naive queue might be considered &quot;unfair&quot; to bursty workloads that have low overall bandwidth utilization because workloads that have low bandwidth utilization &quot;shouldn't&quot; suffer more drops than workloads that are less bursty but use more bandwidth (this is also arguably not unfair, depending on what your goals are).</p> <p>A class of solutions to this problem are <a href=https://en.wikipedia.org/wiki/Random_early_detection>random early drop</a> and its variants, which gives incoming items a probability of being dropped which can be determined by queue fullness (and possibly other factors), smoothing out the discontinuity and mitigating issues caused by having a discontinuous probability of queue drops.</p> <p><a href=//danluu.com/randomize-hn/ >This post on voting in link aggregators</a> is fundamentally the same idea although, in some sense, the polarity is reversed. There's a very sharp discontinuity in how much traffic something gets based on whether or not it's on the front page. You could view this as a link getting dropped from a queue if it only receives N-1 votes and not getting dropped if it receives N votes.</p> <h3 id=college-admissions-and-pell-grant-recipients-https-www-insidehighered-com-admissions-article-2019-01-28-study-pressure-enroll-more-pell-eligible-students-has-skewed-colleges><a href=https://www.insidehighered.com/admissions/article/2019/01/28/study-pressure-enroll-more-pell-eligible-students-has-skewed-colleges>College admissions and Pell Grant recipients</a></h3> <p><a href=https://en.wikipedia.org/wiki/Pell_Grant>Pell Grants</a> started getting used as a proxy for how serious schools are about helping/admitting low-income students. The first order impact is that students above the Pell Grant threshold had a significantly reduced probability of being admitted while students below the Pell Grant threshold had a significantly higher chance of being admitted. Phrased that way, it sounds like things are working as intended.</p> <p>However, when we look at what happens within each group, we see outcomes that are the opposite of what we'd want if the goal is to benefit students from low income families. Among people who don't qualify for a Pell Grant, it's those with the lowest income who are the most severely impacted and have the most severely reduced probability of admission. Among people who do qualify, it's those with the highest income who are mostly likely to benefit, again the opposite of what you'd probably want if your goal is to benefit students from low income families.</p> <p>We can see these in the graphs below, which are histograms of parental income among students at two universities in 2008 (first graph) and 2016 (second graph), where the red line indicates the Pell Grant threshold.</p> <p><img src=/images/discontinuities/pell-2008.jpg alt="Histogram of income distribution of students at two universities in 2008; high incomes are highly overrepresented relative to the general population, but the distribution is smooth"height=816 width=1056></p> <p><img src=/images/discontinuities/pell-2016.jpg alt="Histogram of income distribution of students at two universities in 2016; high incomes are still highly overrepresented, there's also a sharp discontinuity at the Pell grant threshold; plot looks roughly two upwards sloping piecewise linear functions, with a drop back to nearly 0 at the discontinuity at the Pell grant threshold"height=816 width=1056></p> <p>A second order effect of universities optimizing for Pell Grant recipients is that savvy parents can do the same thing that some people do to cut their taxable income at the last minute. Someone might put money into a traditional IRA instead of a Roth IRA and, if they're at their IRA contribution limit, they can try to lose money on options, effectively transferring money to options traders who are likely to be wealthier than them, in order to bring their income below the Pell Grant threshold, increasing the probability that their children will be admitted to a selective school.</p> <h3 id=election-statistics-https-arxiv-org-pdf-1410-6059-pdf><a href=https://arxiv.org/pdf/1410.6059.pdf>Election statistics</a></h3> <p>The following histograms of Russian elections across polling stations shows curious spikes in turnout and results at nice, round, numbers (e.g., 95%) starting around 2004. This appears to indicate that there's election fraud via fabricated results and that at least some of the people fabricating results don't bother with fabricating results that have a smooth distribution.</p> <p><img src=/images/discontinuities/russian-elections.png height=1418 width=1822></p> <p>For finding fraudulent numbers, also see, <a href=https://en.wikipedia.org/wiki/Benford%27s_law>Benford's law</a>.</p> <h3 id=used-car-sale-prices-https-www-ftc-gov-sites-default-files-documents-public-events-3rd-annual-microeconomics-conference-lacetera-slide-pdf><a href=https://www.ftc.gov/sites/default/files/documents/public_events/3rd-annual-microeconomics-conference/lacetera_slide.pdf>Used car sale prices</a></h3> <p><a href=https://twitter.com/ainsworld/status/1436418752409706519>Mark Ainsworth points out that there are discontinuities</a> at $10k boundaries in U.S. auto auction sales prices as well as volume of vehicles offered at auction. The price graph below adjusts for a number of factors such as model year, but we can see the same discontinuities in the raw unadjusted data.</p> <p><img src=/images/discontinuities/car-prices.png alt="Graph of car sales prices at auction, showing discontinuities described above"height=1670 width=942></p> <p><img src=/images/discontinuities/car-volumes.png alt="Graph of car volumes at auction, showing discontinuities described above for dealer sales to auction but not fleet sales to auction"height=1710 width=992></p> <h3 id=p-values-https-en-wikipedia-org-wiki-p-value><a href=https://en.wikipedia.org/wiki/P-value>p-values</a></h3> <p>Authors of psychology papers are incentivized to produce papers with <a href=https://en.wikipedia.org/wiki/P-value>p values</a> below some threshold, usually 0.05, but sometimes 0.1 or 0.01. <a href=https://www.ncbi.nlm.nih.gov/pubmed/22853650>Masicampo et al. plotted p values from papers published in three psychology journals</a> and found a curiously high number of papers with p values just below 0.05.</p> <p><img src=/images/discontinuities/p-value.png alt="Histogram of published p-values; spike at p=0.05"height=1000 width=1874></p> <p>The spike at p = 0.05 consistent with a number of hypothesis that aren't great, such as:</p> <ul> <li>Authors are fudging results to get p = 0.05</li> <li>Journals are much more likely to accept a paper with p = 0.05 than if p = 0.055</li> <li>Authors are much less likely to submit results if p = 0.055 than if p = 0.05</li> </ul> <p><a href=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4359000/ >Head et al. (2015)</a> surveys the evidence across a number of fields.</p> <p>Andrew Gelman and others have been campaigning to get rid of the idea of statistical significance and p-value thresholds for years, <a href=https://stat.columbia.edu/~gelman/research/unpublished/abandon.pdf>see this paper for a short summary of why</a>. Not only would this reduce the incentive for authors to cheat on p values, there are other reasons to not want a bright-line rule to determine if something is &quot;significant&quot; or not.</p> <h3 id=drug-charges-http-econweb-umd-edu-tuttle-files-tuttle-mandatory-minimums-pdf><a href=http://econweb.umd.edu/~tuttle/files/tuttle_mandatory_minimums.pdf>Drug charges</a></h3> <p>The top two graphs in this set of four show histograms of the amount of cocaine people were charged with possessing before and after the passing of the Fair Sentencing Act in 2010, which raised the amount of cocaine necessary to trigger the 10-year mandatory minimum prison sentence for possession from 50g to 280g. There's a relatively smooth distribution before 2010 and a sharp discontinuity after 2010.</p> <p>The bottom-left graph shows the sharp spike in prosecutions at 280 grams followed by what might be a drop in 2013 after evidentiary standards were changed<sup class=footnote-ref id=fnref:R><a rel=footnote href=#fn:R>1</a></sup>.</p> <p><img src=/images/discontinuities/cocaine-280.png height=1118 width=1408></p> <h3 id=high-school-exit-exam-scores-danluu-com-matura-2013-pdf><a href=//danluu.com/matura-2013.pdf>High school exit exam scores</a></h3> <p>This is a histogram of high school exit exam scores from the Polish language exam. We can see that a curiously high number of students score 30 or just above thirty while curiously low number of students score from 23-29. This is from 2013; other years I've looked at (2010-2012) show a similar discontinuity.</p> <p>Math exit exam scores don't exhibit any unusual discontinuities in the years I've examined (2010-2013).</p> <p><img src=/images/discontinuities/matura-polish-2013.png height=772 width=1416></p> <p><a href=https://www.reddit.com/r/dataisbeautiful/comments/1bqf9r/unusual_distributions_of_scores_on_final/c994zxt/ >An anonymous reddit commenter explains this</a>:</p> <blockquote> <p>When a teacher is grading matura (final HS exam), he/she doesn't know whose test it is. The only things that are known are: the number (code) of the student and the district which matura comes from (it is usually from completely different part of Poland). The system is made to prevent any kind of manipulation, for example from time to time teachers supervisor will come to check if test are graded correctly. I don't wanna talk much about system flaws (and advantages), it is well known in every education system in the world where final tests are made, but you have to keep in mind that there is a key, which teachers follow very strictly when grading.</p> <p>So, when a score of the test is below 30%, exam is failed. However, before making final statement in protocol, a commision of 3 (I don't remember exact number) is checking test again. This is the moment, where difference between humanities and math is shown: teachers often try to find a one (or a few) missing points, so the test won't be failed, because it's a tragedy to this person, his school and somewhat fuss for the grading team. Finding a &quot;missing&quot; point is not that hard when you are grading writing or open questions, which is a case in polish language, but nearly impossible in math. So that's the reason why distribution of scores is so different.</p> </blockquote> <p>As with p values, having a bright-line threshold, causes curious behavior. In this case, scoring below 30 on any subject (a 30 or above is required in every subject) and failing the exam has arbitrary negative effects for people, so teachers usually try to prevent people from failing if there's an easy way to do it, but a deeper root of the problem is the idea that it's necessary to produce a certification that's the discretization of a continuous score.</p> <h3 id=birth-month-and-sports>Birth month and sports</h3> <p>These are scatterplots of football (soccer) players in the <a href=https://en.wikipedia.org/wiki/UEFA_Youth_League>UEFA Youth League</a>. The x-axis on both of these plots is how old players are modulo the year, i.e., their birth month normalized from 0 to 1.</p> <p>The graph on the left is a histogram, which shows that there is a very strong relationship between where a person's birth falls within the year and their odds of making a club at the UEFA Youth League (U19) level. The graph on the right purports to show that birth time is only weakly correlated with actual value provided on the field. The authors use playing time as a proxy for value, presumably because it's easy to measure. That's not a great measure, but the result they find (younger-within-the-year players have higher value, conditional on making the U19 league) is consistent with other studies on sports and discrimination, which ind (for example) that <a href=tech-discrimination/ >black baseball players were significantly better than white baseball players for decades after desegregation in baseball, French-Canadian defensemen are also better than average (French-Canadians are stereotypically afraid to fight, don't work hard enough, and are too focused on offense)</a>.</p> <p>The discontinuity isn't directly shown in the graphs above because the graphs only show birth date for one year. If we were to plot birth date by cohort across multiple years, we'd expect to see a sawtooth pattern in the probability that a player makes it into the UEFA youth league with a 10x difference between someone born one day before vs. after the threshold.</p> <p><img src=/images/discontinuities/u19-age.png height=1030 width=1758></p> <p>This phenomenon, that birth day or month is a good predictor of participation in higher-level youth sports as well as pro sports, has been studied across a variety of sports.</p> <p>It's generally believed that this is caused by a discontinuity in youth sports:</p> <ol> <li>Kids are bucketed into groups by age in years and compete against people in the same year</li> <li>Within a given year, older kids are stronger, faster, etc., and perform better</li> <li>This causes older-within-year kids to outcompete younger kids, which later results in older-within-year kids having higher levels of participation for a variety of reasons</li> </ol> <p>This is arguably a &quot;bug&quot; in how youth sports works. But <a href=//danluu.com/bad-decisions/ >as we've seen in baseball</a> <a href=//danluu.com/tech-discrimination/ >as well as a survey of multiple sports</a>, obviously bad decision making that costs individual teams tens or even hundreds of millions of dollars can persist for decades in the face of people pubicly discussing how bad the decisions are. In this case, the youth sports teams aren't feeder teams to pro teams, so they don't have a financial incentive to select players who are skilled for their age (as opposed to just taller and faster because they're slightly older) so this system-wide non-optimal even more difficult to fix than pro sports teams making locally non-optimal decisions that are completely under their control.</p> <h3 id=procurement-auctions-http-www-keikawai-com-full-0804-pdf><a href=http://www.keikawai.com/Full_0804.pdf>Procurement auctions</a></h3> <p>Kawai et al. looked at Japanese government procurement, in order to find suspicious pattern of bids like the ones described in <a href=https://www.nber.org/papers/w4013>Porter et al. (1993)</a>, which looked at collusion in procurement auctions on Long Island (in New York in the United States). One example that's given is:</p> <blockquote> <p>In February 1983, the New York State Department of Transportation (DoT) held a pro- curement auction for resurfacing 0.8 miles of road. The lowest bid in the auction was $4 million, and the DoT decided not to award the contract because the bid was deemed too high relative to its own cost estimates. The project was put up for a reauction in May 1983 in which all the bidders from the initial auction participated. The lowest bid in the reauction was 20% higher than in the initial auction, submitted by the previous low bidder. Again, the contract was not awarded. The DoT held a third auction in February 1984, with the same set of bidders as in the initial auction. The lowest bid in the third auction was 10% higher than the second time, again submitted by the same bidder. The DoT apparently thought this was suspicious: “It is notable that the same firm submitted the low bid in each of the auctions. Because of the unusual bidding patterns, the contract was not awarded through 1987.”</p> </blockquote> <p>It could be argued that this is expected because different firms have different cost structures, so the lowest bidder in an auction for one particular project should be expected to be the lowest bidder in subsequent auctions for the same project. In order to distinguish between collusion and real structural cost differences between firms, Kawai et al. (2015) looked at auctions where the difference in bid between the first and second place firms was very small, making the winner effectively random.</p> <p>In the auction structure studied, bidders submit a secret bid. If the secret bid is above a secret minimum, then the lowest bidder wins the auction and gets the contract. If not, the lowest bid is revealed to all bidders and another round of bidding is done. Kawai et al. found that, in about 97% of auctions, the bidder who submitted the lowest bid in the first round also submitted the lowest bid in the second round (the probability that the second lowest bidder remains second lowest was 26%).</p> <p>Below, is a histogram of the difference in first and second round bids between the first-lowest and second-lowest bidders (left column) and the second-lowest and third-lowest bidders (right column). Each row has a different filtering criteria for how close the auction has to be in order to be included. In the top row, all auctions that reached the third round were included; in second, and third rows, the normalized delta between the first and second biders was less than 0.05 and 0.01, respectively; in the last row, the normalized delta between the first and the third bidder was less than 0.03. All numbers are normalized because the absolute size of auctions can vary.</p> <p><img src=/images/discontinuities/jp-procurement-bidding.png height=1446 width=1682></p> <p>We can see that the distributions of deltas between the first and second round are roughly symmetrical when comparing second and third lowest bidders. But when comparing first and second lowest bidders, there's a sharp discontinuity at zero, indicating that second-lowest bidder almost never lowers their bid by more than the first-lower bidder did. If you read the paper, you can see that the same structure persists into auctions that go into a third round.</p> <p>I don't mean to pick on Japanese procurement auctions in particular. There's an extensive literature on procurement auctions that's found collusion in many cases, often much more blatant than the case presented above (e.g., there are a few firms and they round-robin who wins across auctions, or there are a handful of firms and every firm except for the winner puts in the same losing bid).</p> <h3 id=restaurant-inspection-https-iquantny-tumblr-com-post-76928412519-think-nyc-restaurant-grading-is-flawed-heres-scores-http-datafra-me-blog-calling-out-nyc-restaurant-violations><a href=https://iquantny.tumblr.com/post/76928412519/think-nyc-restaurant-grading-is-flawed-heres>Restaurant inspection</a> <a href=http://datafra.me/blog/Calling-out-NYC-restaurant-violations>scores</a></h3> <p>The histograms below show a sharp discontinuity between 13 and 14, which is the difference between an A grade and a B grade. It appears that some regions also have a discontinuity between 27 and 28, which is the difference between a B and a C and <a href=https://iquantny.tumblr.com/post/76928412519/think-nyc-restaurant-grading-is-flawed-heres>this older analysis from 2014</a> found what appears to be a similar discontinuity between B and C grades.</p> <p><img src=/images/discontinuities/nyc-restaurant-inspections.png height=900 width=1260></p> <p>Inspectors have discretion in what violations are tallied and it appears that there are cases where restaurant are nudged up to the next higher grade.</p> <h3 id=marathon-finishing-times-https-faculty-chicagobooth-edu-devin-pope-research-pdf-website-marathons-pdf><a href=https://faculty.chicagobooth.edu/devin.pope/research/pdf/Website_Marathons.pdf>Marathon finishing times</a></h3> <p>A histogram of marathon finishing times (finish times on the x-axis, count on the y-axis) across 9,789,093 finishes shows noticeable discontinuities at every half hour, as well as at &quot;round&quot; times like :10, :15, and :20.</p> <p><img src=/images/discontinuities/marathon-times.png height=1094 width=1530></p> <p>An analysis of times within each race (<a href=faculty.chicagobooth.edu/devin.pope/research/pdf/Website_Marathons.pdf>see section 4.4, figures 7-9</a>) indicates that this is at least partially because people speed up (or slow down less than usual) towards the end of races if they're close to a &quot;round&quot; time<sup class=footnote-ref id=fnref:M><a rel=footnote href=#fn:M>2</a></sup>.</p> <h3 id=notes>Notes</h3> <p>This post doesn't really have a goal or a point, it's just a collection of discontinuities that I find fun.</p> <p>One thing that's maybe worth noting is that I've gotten a lot of mileage out in my career both out of being suspicious of discontinuities and figuring out where they come from and also out of applying standard techniques to smooth out discontinuities.</p> <p>For finding discontinuities, basic tools like &quot;drawing a scatterplot&quot;, &quot;<a href=//danluu.com/perf-tracing/#histogram>drawing a histogram</a>&quot;, &quot;drawing the <a href=https://en.wikipedia.org/wiki/Cumulative_distribution_function>CDF</a>&quot; often come in handy. Other kinds of visualizations that add temporality, like <a href=https://github.com/Netflix/flamescope>flamescope</a>, can also come in handy.</p> <p>We <a href=#hardware-or-software-queues>noted above that queues create a kind of discontinuity that, in some circumstances, should be smoothed out</a>. We also noted that we see similar behavior for other kinds of thresholds and that randomization can be a useful tool to smooth out discontinuities in thresholds as well. Randomization can also be used to allow for reducing quantization error when reducing precision with ML and in other applications.</p> <p><small>Thanks to Leah Hanson, Omar Rizwan, Dmitry Belenko, Kamal Marhubi, Danny Vilea, Nick Roberts, Lifan Zeng, Mark Ainsworth, Wesley Aptekar-Cassels, Thomas Hauk, @BaudDev, and Michael Sullivan for comments/corrections/discussion.</small></p> <p>Also, please feel free to <a href=https://twitter.com/danluu/status/1230595642390564866>send me other interesting discontinuities</a>! </p> <p><link rel=prefetch href=//danluu.com/randomize-hn/ > <link rel=prefetch href=//danluu.com/bad-decisions/ > <link rel=prefetch href=//danluu.com/tech-discrimination/ ></p> <div class=footnotes> <hr> <ol> <li id=fn:R><p>Most online commentary I've seen about this paper is incorrect. I've seen this paper used as evidence of police malfeasance because the amount of cocaine seized jumped to 280g. This is the opposite of what's described in the paper, where the author notes that, based on drug seizure records, amounts seized do not appear to be the cause of this change. After noting that drug seizures are not the cause, the author notes that prosecutors can charge people for amounts that are not the same as the amount seized and then notes:</p> <blockquote> <p>I do find bunching at 280g after 2010 in case management data from the Executive Office of the US Attorney (EOUSA). I also find that approximately 30% of prosecutors are responsible for the rise in cases with 280g after 2010, and that there is variation in prosecutor-level bunching both within and between districts. Prosecutors who bunch cases at 280g also have a high share of cases right above 28g after 2010 (the 5-year threshold post-2010) and a high share of cases above 50g prior to 2010 (the 10-year threshold pre-2010). Also, bunching above a mandatory minimum threshold persists across districts for prosecutors who switch districts. Moreover, when a “bunching” prosecutor switches into a new district, all other attorneys in that district increase their own bunching at mandatory minimums. These results suggest that the observed bunching at sentencing is specifically due to prosecutorial discretion</p> </blockquote> <p>This is mentioned in the abstract and then expounded on in the introduction (the quoted passage is from the introduction), so I think that most people commenting on this paper can't have read it. I've done a few surveys of comments on papers on blog posts and I generally find that, in cases where it's possible to identify this (e.g., when the post is mistitled), the vast majority of commenters can't have read the paper or post they're commenting on, but that's a topic for another post.</p> <p>There is some evidence that something fishy may be going on in seizures (e.g., see Fig. A8.(c)), but if the analysis in the paper is correct, that impact of that is much smaller than the impact of prosecutorial discretion.</p> <a class=footnote-return href=#fnref:R><sup>[return]</sup></a></li> <li id=fn:M>One of the most common comments I've seen online about this graph and/or this paper is that this is due to pace runners provided by the marathon. Section 4.4 of the paper gives multiple explanations for why this cannot be the case, once again indicating that people tend to comment without reading the paper. <a class=footnote-return href=#fnref:M><sup>[return]</sup></a></li> </ol> </div> </main><nav><div class=np> <a href=cli-complexity/ >← The growth of command line options, 1979-Present</a> <a href=p95-skill/ >95%-ile isn&#39;t that good →</a> </div></nav> <footer><div class=np> <a href= >Archive</a> <a href=https://mastodon.social/@danluu>Mastodon</a> <a href=https://threads.net/@danluu.danluu>Threads</a> </div> <div class=np> <a href=https://www.patreon.com/danluu>Patreon</a> <a href=https://www.linkedin.com/in/danluu/ >LinkedIn</a> <a href=https://twitter.com/danluu/ >Twitter</a> <a href=atom.xml>RSS</a></div></footer> 