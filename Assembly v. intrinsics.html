<!doctypehtml><html lang=en><meta charset=utf-8><title>Assembly v. intrinsics</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="stylesheet" href="/styles.css"></style><link rel=icon href="data:;base64,="> <header><strong>Assembly v. intrinsics</strong> | <i><b><a href=https://patreon.com/danluu>I'm trying some experimental tiers on Patreon</a></b> to see if I can get to <a href=https://twitter.com/danluu/status/1456346963691991041>substack-like levels of financial support for this blog without moving to substack</a>!</i><hr></header><main> <p>Every once in a while, I hear how intrinsics have improved enough that it's safe to use them for high performance code. That would be nice. The promise of intrinsics is that you can write optimized code by calling out to functions (intrinsics) that correspond to particular assembly instructions. Since intrinsics act like normal functions, they can be cross platform. And since your compiler has access to more computational power than your brain, as well as a detailed model of every CPU, the compiler should be able to do a better job of micro-optimizations. Despite decade old claims that intrinsics can make your life easier, it never seems to work out.</p> <p>The last time I tried intrinsics was around 2007; for more on why they were hopeless then (<a href="http://virtualdub.org/blog/pivot/entry.php?id=162">see this exploration by the author of VirtualDub</a>). I gave them another shot recently, and while they've improved, they're still not worth the effort. The problem is that intrinsics are so unreliable that you have to manually check the result on every platform and every compiler you expect your code to be run on, and then tweak the intrinsics until you get a reasonable result. That's more work than just writing the assembly by hand. If you don't check the results by hand, it's easy to get bad results.</p>  <p>For example, as of this writing, the first two Google hits for <code>popcnt benchmark</code> (and 2 out of the top 3 bing hits) claim that Intel's hardware <code>popcnt</code> instruction is slower than a software implementation that counts the number of bits set in a buffer, via a table lookup using the <a href=https://en.wikipedia.org/wiki/SSSE3>SSSE3</a> <code>pshufb</code> instruction. This turns out to be untrue, but it must not be obvious, or this claim wouldn't be so persistent. Let's see why someone might have come to the conclusion that the <code>popcnt</code> instruction is slow if they coded up a solution using intrinsics.</p> <p>One of the top search hits has sample code and benchmarks for both native <code>popcnt</code> as well as the software version using <code>pshufb</code>. <a href=http://www.strchr.com/media/crc32_popcnt.zip>Their code</a> requires MSVC, which I don't have access to, but their first <code>popcnt</code> implementation just calls the <code>popcnt</code> intrinsic in a loop, which is fairly easy to reproduce in a form that gcc and clang will accept. Timing it is also pretty simple, since we're just timing a function (that happens to count the number of bits set in some fixed sized buffer).</p> <pre><code>uint32_t builtin_popcnt(const uint64_t* buf, int len) {
  int cnt = 0;
  for (int i = 0; i &lt; len; ++i) {
    cnt += __builtin_popcountll(buf[i]);
  }
  return cnt;
}
</code></pre> <p>This is slightly different from the code I linked to above, since they use the dword (32-bit) version of <code>popcnt</code>, and we're using the qword (64-bit) version. Since our version gets twice as much done per loop iteration, I'd expect our version to be faster than their version.</p> <p>Running <code>clang -O3 -mpopcnt -funroll-loops</code> produces a binary that we can examine. On macs, we can use <code>otool -tv</code> to get the disassembly. On linux, there's <code>objdump -d</code>.</p> <pre><code>_builtin_popcnt:
; address                        instruction
0000000100000b30        pushq   %rbp
0000000100000b31        movq    %rsp, %rbp
0000000100000b34        movq    %rdi, -0x8(%rbp)
0000000100000b38        movl    %esi, -0xc(%rbp)
0000000100000b3b        movl    $0x0, -0x10(%rbp)
0000000100000b42        movl    $0x0, -0x14(%rbp)
0000000100000b49        movl    -0x14(%rbp), %eax
0000000100000b4c        cmpl    -0xc(%rbp), %eax
0000000100000b4f        jge     0x100000bd4
0000000100000b55        movslq  -0x14(%rbp), %rax
0000000100000b59        movq    -0x8(%rbp), %rcx
0000000100000b5d        movq    (%rcx,%rax,8), %rax
0000000100000b61        movq    %rax, %rcx
0000000100000b64        shrq    %rcx
0000000100000b67        movabsq $0x5555555555555555, %rdx
0000000100000b71        andq    %rdx, %rcx
0000000100000b74        subq    %rcx, %rax
0000000100000b77        movabsq $0x3333333333333333, %rcx
0000000100000b81        movq    %rax, %rdx
0000000100000b84        andq    %rcx, %rdx
0000000100000b87        shrq    $0x2, %rax
0000000100000b8b        andq    %rcx, %rax
0000000100000b8e        addq    %rax, %rdx
0000000100000b91        movq    %rdx, %rax
0000000100000b94        shrq    $0x4, %rax
0000000100000b98        addq    %rax, %rdx
0000000100000b9b        movabsq $0xf0f0f0f0f0f0f0f, %rax
0000000100000ba5        andq    %rax, %rdx
0000000100000ba8        movabsq $0x101010101010101, %rax
0000000100000bb2        imulq   %rax, %rdx
0000000100000bb6        shrq    $0x38, %rdx
0000000100000bba        movl    %edx, %esi
0000000100000bbc        movl    -0x10(%rbp), %edi
0000000100000bbf        addl    %esi, %edi
0000000100000bc1        movl    %edi, -0x10(%rbp)
0000000100000bc4        movl    -0x14(%rbp), %eax
0000000100000bc7        addl    $0x1, %eax
0000000100000bcc        movl    %eax, -0x14(%rbp)
0000000100000bcf        jmpq    0x100000b49
0000000100000bd4        movl    -0x10(%rbp), %eax
0000000100000bd7        popq    %rbp
0000000100000bd8        ret
</code></pre> <p>Well, that's interesting. Clang seems to be calculating things manually rather than using <code>popcnt</code>. It seems to be using <a href=http://graphics.stanford.edu/~seander/bithacks.html#CountBitsSetParallel>the approach described here</a>, which is something like</p> <pre><code>x = x - ((x &gt;&gt; 0x1) &amp; 0x5555555555555555);
x = (x &amp; 0x3333333333333333) + ((x &gt;&gt; 0x2) &amp; 0x3333333333333333);
x = (x + (x &gt;&gt; 0x4)) &amp; 0xF0F0F0F0F0F0F0F;
ans = (x * 0x101010101010101) &gt;&gt; 0x38;
</code></pre> <p>That's not bad for a simple implementation that doesn't rely on any kind of specialized hardware, but that's going to take a lot longer than a single <code>popcnt</code> instruction.</p> <p>I've got a pretty old version of clang (3.0), so let me try this again after upgrading to 3.4, in case they added hardware <code>popcnt</code> support “recently”.</p> <pre><code>0000000100001340        pushq   %rbp         ; save frame pointer
0000000100001341        movq    %rsp, %rbp   ; new frame pointer
0000000100001344        xorl    %ecx, %ecx   ; cnt = 0
0000000100001346        testl   %esi, %esi
0000000100001348        jle     0x100001363
000000010000134a        nopw    (%rax,%rax)
0000000100001350        popcntq (%rdi), %rax ; “eax” = popcnt[rdi]
0000000100001355        addl    %ecx, %eax   ; eax += cnt
0000000100001357        addq    $0x8, %rdi   ; increment address by 64-bits (8 bytes)
000000010000135b        decl    %esi         ; decrement loop counter; sets flags
000000010000135d        movl    %eax, %ecx   ;  cnt = eax; does not set flags
000000010000135f        jne     0x100001350  ; examine flags. if esi != 0, goto popcnt
0000000100001361        jmp     0x100001365  ; goto “restore frame pointer”
0000000100001363        movl    %ecx, %eax
0000000100001365        popq    %rbp         ; restore frame pointer
0000000100001366        ret

</code></pre> <p>That's better! We get a hardware <code>popcnt</code>! Let's compare this to the SSSE3 <code>pshufb</code> implementation <a href=http://www.strchr.com/crc32_popcnt>presented here</a> as the fastest way to do a popcnt. We'll use a table like the one in the link to show speed, except that we're going to show a rate, instead of the raw cycle count, so that the relative speed between different sizes is clear. The rate is GB/s, i.e., how many gigs of buffer we can process per second. We give the function data in chunks (varying from 1kb to 16Mb); each column is the rate for a different chunk-size. If we look at how fast each algorithm is for various buffer sizes, we get the following.</p> <table> <thead> <tr> <th>Algorithm</th> <th>1k</th> <th>4k</th> <th>16k</th> <th>65k</th> <th>256k</th> <th>1M</th> <th>4M</th> <th>16M</th> </tr> </thead> <tbody> <tr> <td>Intrinsic</td> <td>6.9</td> <td>7.3</td> <td>7.4</td> <td>7.5</td> <td>7.5</td> <td>7.5</td> <td>7.5</td> <td>7.5</td> </tr> <tr> <td>PSHUFB</td> <td>11.5</td> <td>13.0</td> <td>13.3</td> <td>13.4</td> <td>13.1</td> <td>13.4</td> <td>13.0</td> <td>12.6</td> </tr> </tbody> </table> <p>That's not so great. Relative to the the benchmark linked above, we're doing better because we're using 64-bit <code>popcnt</code> instead of 32-bit <code>popcnt</code>, but the PSHUFB version is still almost twice as fast<sup class=footnote-ref id=fnref:B><a rel=footnote href=#fn:B>1</a></sup>.</p> <p>One odd thing is the way <code>cnt</code> gets accumulated. <code>cnt</code> is stored in <code>ecx</code>. But, instead of adding the result of the <code>popcnt</code> to <code>ecx</code>, clang has decided to add <code>ecx</code> to the result of the <code>popcnt</code>. To fix that, clang then has to move that sum into <code>ecx</code> at the end of each loop iteration.</p> <p>The other noticeable problem is that we only get one <code>popcnt</code> per iteration of the loop, which means the loop isn't getting unrolled, and we're paying the entire cost of the loop overhead for each <code>popcnt</code>. Unrolling the loop can also let the CPU extract more instruction level parallelism from the code, although that's a bit beyond the scope of this blog post.</p> <p>Using clang, that happens even with <code>-O3 -funroll-loops</code>. Using gcc, we get a properly unrolled loop, but gcc has other problems, as we'll see later. For now, let's try unrolling the loop ourselves by calling <code>__builtin_popcountll</code> multiple times during each iteration of the loop. For simplicity, let's try doing four <code>popcnt</code> operations on each iteration. I don't claim that's optimal, but it should be an improvement.</p> <pre><code>uint32_t builtin_popcnt_unrolled(const uint64_t* buf, int len) {
  assert(len % 4 == 0);
  int cnt = 0;
  for (int i = 0; i &lt; len; i+=4) {
    cnt += __builtin_popcountll(buf[i]);
    cnt += __builtin_popcountll(buf[i+1]);
    cnt += __builtin_popcountll(buf[i+2]);
    cnt += __builtin_popcountll(buf[i+3]);
  }
  return cnt;
}
</code></pre> <p>The core of our loop now has</p> <pre><code>0000000100001390        popcntq (%rdi,%rcx,8), %rdx
0000000100001396        addl    %eax, %edx
0000000100001398        popcntq 0x8(%rdi,%rcx,8), %rax
000000010000139f        addl    %edx, %eax
00000001000013a1        popcntq 0x10(%rdi,%rcx,8), %rdx
00000001000013a8        addl    %eax, %edx
00000001000013aa        popcntq 0x18(%rdi,%rcx,8), %rax
00000001000013b1        addl    %edx, %eax
</code></pre> <p>with pretty much the same code surrounding the loop body. We're doing four <code>popcnt</code> operations every time through the loop, which results in the following performance:</p> <table> <thead> <tr> <th>Algorithm</th> <th>1k</th> <th>4k</th> <th>16k</th> <th>65k</th> <th>256k</th> <th>1M</th> <th>4M</th> <th>16M</th> </tr> </thead> <tbody> <tr> <td>Intrinsic</td> <td>6.9</td> <td>7.3</td> <td>7.4</td> <td>7.5</td> <td>7.5</td> <td>7.5</td> <td>7.5</td> <td>7.5</td> </tr> <tr> <td>PSHUFB</td> <td>11.5</td> <td>13.0</td> <td>13.3</td> <td>13.4</td> <td>13.1</td> <td>13.4</td> <td>13.0</td> <td>12.6</td> </tr> <tr> <td>Unrolled</td> <td>12.5</td> <td>14.4</td> <td>15.0</td> <td>15.1</td> <td>15.2</td> <td>15.2</td> <td>15.2</td> <td>15.2</td> </tr> </tbody> </table> <p>Between using 64-bit <code>popcnt</code> and unrolling the loop, we've already beaten the allegedly faster <code>pshufb</code> code! But it's close enough that we might get different results with another compiler or some other chip. Let's see if we can do better.</p> <p>So, what's the deal with this <a href=http://stackoverflow.com/questions/25078285/replacing-a-32-bit-loop-count-variable-with-64-bit-introduces-crazy-performance>popcnt false dependency bug</a> that's been getting a lot of publicity lately? Turns out, <code>popcnt</code> has a false dependency on its destination register, which means that even though the result of <code>popcnt</code> doesn't depend on its destination register, the CPU thinks that it does and will wait until the destination register is ready before starting the <code>popcnt</code> instruction.</p> <p>x86 typically has two operand operations, e.g., <code>addl %eax, %edx</code> adds <code>eax</code> and <code>edx</code>, and then places the result in <code>edx</code>, so it's common for an operation to have a dependency on its output register. In this case, there shouldn't be a dependency, since the result doesn't depend on the contents of the output register, but that's an easy bug to introduce, and a hard one to catch<sup class=footnote-ref id=fnref:D><a rel=footnote href=#fn:D>2</a></sup>.</p> <p>In this particular case, <code>popcnt</code> has a 3 cycle latency, but it's pipelined such that a <code>popcnt</code> operation can execute each cycle. If we ignore other overhead, that means that a single <code>popcnt</code> will take 3 cycles, 2 will take 4 cycles, 3 will take 5 cycles, and n will take n+2 cycles, as long as the operations are independent. But, if the CPU incorrectly thinks there's a dependency between them, we effectively lose the ability to pipeline the instructions, and that n+2 turns into 3n.</p> <p>We can work around this by buying a CPU from AMD or VIA, or by putting the <code>popcnt</code> results in different registers. Let's making an array of destinations, which will let us put the result from each <code>popcnt</code> into a different place.</p> <pre><code>uint32_t builtin_popcnt_unrolled_errata(const uint64_t* buf, int len) {
  assert(len % 4 == 0);
  int cnt[4];
  for (int i = 0; i &lt; 4; ++i) {
    cnt[i] = 0;
  }

  for (int i = 0; i &lt; len; i+=4) {
    cnt[0] += __builtin_popcountll(buf[i]);
    cnt[1] += __builtin_popcountll(buf[i+1]);
    cnt[2] += __builtin_popcountll(buf[i+2]);
    cnt[3] += __builtin_popcountll(buf[i+3]);
  }
  return cnt[0] + cnt[1] + cnt[2] + cnt[3];
}
</code></pre> <p>And now we get</p> <pre><code>0000000100001420        popcntq (%rdi,%r9,8), %r8
0000000100001426        addl    %ebx, %r8d
0000000100001429        popcntq 0x8(%rdi,%r9,8), %rax
0000000100001430        addl    %r14d, %eax
0000000100001433        popcntq 0x10(%rdi,%r9,8), %rdx
000000010000143a        addl    %r11d, %edx
000000010000143d        popcntq 0x18(%rdi,%r9,8), %rcx
</code></pre> <p>That's better -- we can see that the first popcnt outputs into <code>r8</code>, the second into <code>rax</code>, the third into <code>rdx</code>, and the fourth into <code>rcx</code>. However, this does the same odd accumulation as the original, where instead of adding the result of the <code>popcnt</code> to <code>cnt[i]</code>, it does the opposite, which necessitates moving the results back to <code>cnt[i]</code> afterwards.</p> <pre><code>000000010000133e        movl    %ecx, %r10d
0000000100001341        movl    %edx, %r11d
0000000100001344        movl    %eax, %r14d
0000000100001347        movl    %r8d, %ebx
</code></pre> <p>Well, at least in clang (3.4). Gcc (4.8.2) is too smart to fall for this separate destination thing and “optimizes” the code back to something like our original version.</p> <table> <thead> <tr> <th>Algorithm</th> <th>1k</th> <th>4k</th> <th>16k</th> <th>65k</th> <th>256k</th> <th>1M</th> <th>4M</th> <th>16M</th> </tr> </thead> <tbody> <tr> <td>Intrinsic</td> <td>6.9</td> <td>7.3</td> <td>7.4</td> <td>7.5</td> <td>7.5</td> <td>7.5</td> <td>7.5</td> <td>7.5</td> </tr> <tr> <td>PSHUFB</td> <td>11.5</td> <td>13.0</td> <td>13.3</td> <td>13.4</td> <td>13.1</td> <td>13.4</td> <td>13.0</td> <td>12.6</td> </tr> <tr> <td>Unrolled</td> <td>12.5</td> <td>14.4</td> <td>15.0</td> <td>15.1</td> <td>15.2</td> <td>15.2</td> <td>15.2</td> <td>15.2</td> </tr> <tr> <td>Unrolled 2</td> <td>14.3</td> <td>16.3</td> <td>17.0</td> <td>17.2</td> <td>17.2</td> <td>17.0</td> <td>16.8</td> <td>16.7</td> </tr> </tbody> </table> <p>To get a version that works with both gcc and clang, and doesn't have these extra <code>mov</code>s, we'll have to write the assembly by hand<sup class=footnote-ref id=fnref:A><a rel=footnote href=#fn:A>3</a></sup>:</p> <pre><code>uint32_t builtin_popcnt_unrolled_errata_manual(const uint64_t* buf, int len) {
  assert(len % 4 == 0);
  uint64_t cnt[4];
  for (int i = 0; i &lt; 4; ++i) {
    cnt[i] = 0;
  }

  for (int i = 0; i &lt; len; i+=4) {
    __asm__(
        &quot;popcnt %4, %4  \n\
        &quot;add %4, %0     \n\t&quot;
        &quot;popcnt %5, %5  \n\t&quot;
        &quot;add %5, %1     \n\t&quot;
        &quot;popcnt %6, %6  \n\t&quot;
        &quot;add %6, %2     \n\t&quot;
        &quot;popcnt %7, %7  \n\t&quot;
        &quot;add %7, %3     \n\t&quot; // +r means input/output, r means intput
        : &quot;+r&quot; (cnt[0]), &quot;+r&quot; (cnt[1]), &quot;+r&quot; (cnt[2]), &quot;+r&quot; (cnt[3])
        : &quot;r&quot;  (buf[i]), &quot;r&quot;  (buf[i+1]), &quot;r&quot;  (buf[i+2]), &quot;r&quot;  (buf[i+3]));
  }
  return cnt[0] + cnt[1] + cnt[2] + cnt[3];
}

</code></pre> <p>This directly translates the assembly into the loop:</p> <pre><code>00000001000013c3        popcntq %r10, %r10
00000001000013c8        addq    %r10, %rcx
00000001000013cb        popcntq %r11, %r11
00000001000013d0        addq    %r11, %r9
00000001000013d3        popcntq %r14, %r14
00000001000013d8        addq    %r14, %r8
00000001000013db        popcntq %rbx, %rbx
</code></pre> <p>Great! The <code>add</code>s are now going the right direction, because we specified exactly what they should do.</p> <table> <thead> <tr> <th>Algorithm</th> <th>1k</th> <th>4k</th> <th>16k</th> <th>65k</th> <th>256k</th> <th>1M</th> <th>4M</th> <th>16M</th> </tr> </thead> <tbody> <tr> <td>Intrinsic</td> <td>6.9</td> <td>7.3</td> <td>7.4</td> <td>7.5</td> <td>7.5</td> <td>7.5</td> <td>7.5</td> <td>7.5</td> </tr> <tr> <td>PSHUFB</td> <td>11.5</td> <td>13.0</td> <td>13.3</td> <td>13.4</td> <td>13.1</td> <td>13.4</td> <td>13.0</td> <td>12.6</td> </tr> <tr> <td>Unrolled</td> <td>12.5</td> <td>14.4</td> <td>15.0</td> <td>15.1</td> <td>15.2</td> <td>15.2</td> <td>15.2</td> <td>15.2</td> </tr> <tr> <td>Unrolled 2</td> <td>14.3</td> <td>16.3</td> <td>17.0</td> <td>17.2</td> <td>17.2</td> <td>17.0</td> <td>16.8</td> <td>16.7</td> </tr> <tr> <td>Assembly</td> <td>17.5</td> <td>23.7</td> <td>25.3</td> <td>25.3</td> <td>26.3</td> <td>26.3</td> <td>25.3</td> <td>24.3</td> </tr> </tbody> </table> <p>Finally! A version that blows away the <code>PSHUFB</code> implementation. How do we know this should be the final version? We can see from <a href=http://www.agner.org/optimize/instruction_tables.pdf>Agner's instruction tables</a> that we can execute, at most, one <code>popcnt</code> per cycle. I happen to have run this on a 3.4Ghz Sandy Bridge, so we've got an upper bound of <code>8 bytes / cycle * 3.4 G cycles / sec = 27.2 GB/s</code>. That's pretty close to the <code>26.3 GB/s</code> we're actually getting, which is a sign that we can't make this much faster<sup class=footnote-ref id=fnref:S><a rel=footnote href=#fn:S>4</a></sup>.</p> <p>In this case, the hand coded assembly version is about 3x faster than the original intrinsic loop (not counting the version from a version of clang that didn't emit a <code>popcnt</code>). It happens that, for the compiler we used, the unrolled loop using the <code>popcnt</code> intrinsic is a bit faster than the <code>pshufb</code> version, but that wasn't true of one of the two unrolled versions when I tried this with <code>gcc</code>.</p> <p>It's easy to see why someone might have benchmarked the same code and decided that <code>popcnt</code> isn't very fast. It's also easy to see why using intrinsics for performance critical code can be a huge time sink<sup class=footnote-ref id=fnref:1><a rel=footnote href=#fn:1>5</a></sup>.</p> <p><small>Thanks to <a href=https://github.com/graue/ >Scott</a> for some comments on the organization of this post, and to <a href=http://blog.leahhanson.us/ >Leah</a> for extensive comments on just about everything</small></p> <p><strong>If you liked this, you'll probably enjoy <a href=//danluu.com/new-cpu-features/ >this post about how CPUs have changed since the 80s</a>.</strong></p> <div class=footnotes> <hr> <ol> <li id=fn:B><a href=https://github.com/danluu/dump/blob/master/popcnt-speed-comparison/popcnt.c>see this</a> for the actual benchmarking code. On second thought, it's an embarrassingly terrible hack, and I'd prefer that you don't look. <a class=footnote-return href=#fnref:B><sup>[return]</sup></a></li> <li id=fn:D>If it were the other way around, and the hardware didn't realize there was a dependency when there should be, that would be easy to catch -- any sequence of instructions that was dependent might produce an incorrect result. In this case, some sequences of instructions are just slower than they should be, which is not trivial to check for. <a class=footnote-return href=#fnref:D><sup>[return]</sup></a></li> <li id=fn:A>This code is a simplified version of <a href=http://stackoverflow.com/questions/25078285/replacing-a-32-bit-loop-count-variable-with-64-bit-introduces-crazy-performance>Alex Yee's stackoverflow answer about the popcnt false dependency bug</a> <a class=footnote-return href=#fnref:A><sup>[return]</sup></a></li> <li id=fn:S><p>That's not quite right, since the CPU has TurboBoost, but it's pretty close. Putting that aside, this example is pretty simple, but calculating this stuff by hand can get tedious for more complicated code. Luckily, the <a href=https://software.intel.com/en-us/articles/intel-architecture-code-analyzer/ >Intel Architecture Code Analyzier</a> can figure this stuff out for us. It finds the bottleneck in the code (assuming infinite memory bandwidth at zero latency), and displays how and why the processor is bottlenecked, which is usually enough to determine if there's room for more optimization.</p> <p>You might have noticed that the performance decreases as the buffer size becomes larger than our cache. It's possible to do a back of the envelope calculation to find the upper bound imposed by the limits of memory and cache performance, but working through the calculations would take a lot more space this this footnote has available to it. You can see a good example of how do it for one simple case <a href=https://software.intel.com/en-us/forums/topic/480004>here</a>. The comments by Nathan Kurz and John McCaplin are particularly good.</p> <a class=footnote-return href=#fnref:S><sup>[return]</sup></a></li> <li id=fn:1><p>In the course of running these benchmarks, I also noticed that <code>_mm_cvtsi128_si64</code> produces bizarrely bad code on gcc (although it's fine in clang). <code>_mm_cvtsi128_si64</code> is the intrinsic for moving an SSE (SIMD) register to a general purpose register (GPR). The compiler has a lot of latitude over whether or not a variable should live in a register or in memory. Clang realizes that it's probably faster to move the value from an SSE register to a GPR if the result is about to get used. Gcc decides to save a register and move the data from the SSE register to memory, and then have the next instruction operate on memory, if that's possible. In our <code>popcnt</code> example, clang uses about 2x for not unrolling the loop, and the rest comes from not being up to date on a CPU bug, which is understandable. It's hard to imagine why a compiler would do a register to memory move when it's about to operate on data unless it either doesn't do optimizations at all, or it has some bug which makes it unaware of the register to register version of the instruction. But at least it gets the right result, <a href="https://social.msdn.microsoft.com/Forums/en-US/b2e688e6-1d28-4cf0-9880-735e6838db6a/a-bug-in-vc-compiler?forum=vsprereleaseannouncements">unlike this version of MSVC</a>.</p> <p>icc and armcc are reputed to be better at dealing with intrinsics, but they're non starters for most open source projects. Downloading icc's free non-commercial version has been disabled for the better part of a year, and even if it comes back, who's going to trust that it won't disappear again? As for armcc, I'm not sure it's ever had a free version?</p> <a class=footnote-return href=#fnref:1><sup>[return]</sup></a></li> </ol> </div> </main><nav><div class=np> <a href=tests-v-reason/ >← Testing v. informal reasoning</a> <a href=google-wage-fixing/ >Google wage fixing, 11-CV-02509-LHK, ORDER DENYING PLAINTIFFS&#39; MOTION FOR PRELIMINARY APPROVAL OF SETTLEMENTS WITH ADOBE, APPLE, GOOGLE, AND INTEL →</a> </div> <div class=np> <a href= >Archive</a> <a href=https://www.patreon.com/danluu>Patreon</a> <a href=https://mastodon.social/@danluu>Mastodon</a> <a href=https://threads.net/@danluu.danluu>Threads</a> </div></nav>