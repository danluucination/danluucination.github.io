<!doctypehtml><html lang=en><meta charset=utf-8><title>We used to build steel mills near cheap power. Now that&#39;s where we build datacenters</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="stylesheet" href="/styles.css"></style><link rel=icon href="data:;base64,="> <header><strong>We used to build steel mills near cheap power. Now that&#39;s where we build datacenters</strong> | <i><b><a href=https://patreon.com/danluu>I'm trying some experimental tiers on Patreon</a></b> to see if I can get to <a href=https://twitter.com/danluu/status/1456346963691991041>substack-like levels of financial support for this blog without moving to substack</a>!</i><hr></header><main> <p>Why are people so concerned with hardware power consumption nowadays? Some common answers to this question are that <a href=http://www.vox.com/2015/3/9/8178213/apple-macbook-all-batteries>power is critically important for phones, tablets, and laptops</a> and that <a href=ftp://ftp.cs.utexas.edu/pub/dburger/papers/ISCA11.pdf>we can put more silicon on a modern chip than we can effectively use</a>. In 2001 <a href="http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=912412&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D912412">Patrick Gelsinger observed that if scaling continued at then-current rates</a>, chips would have the power density of a nuclear reactor by 2005, a rocket nozzle by 2010, and the surface of the sun by 2015, implying that power density couldn't continue on its then-current path. Although this was already fairly obvious at the time, now that it's 2015, we can be extra sure that power density didn't continue to grow at unbounded rates. Anyway, the importance of portables and scaling limits are both valid and important reasons, but since they're widely discussed, I'm going to talk about an underrated reason.</p> <p>People often focus on the portable market because it's cannibalizing desktop market, but that's not the only growth market -- servers are also becoming more important than desktops, and power is really important for servers. To see why power is important for servers, let's look at some calculations about how what it costs to run a datacenter from <a href="http://www.amazon.com/gp/product/012383872X/ref=as_li_qf_sp_asin_il_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=012383872X&amp;linkCode=as2&amp;tag=abroaview-20&amp;linkId=Y6Z2OBCUCR72ALEB">Hennessy &amp; Patterson</a>.</p>  <p>One of the issues is that you pay for power multiple times. Some power is lost at the substation, although we might not have to pay for that directly. Then we lose more storing energy in a UPS. This figure below states 6%, but smaller scale datacenters can easily lose twice that. After that, we lose more power stepping down the power to a voltage that a server can accept. That's over a 10% loss for a setup that's pretty efficient.</p> <p>After that, we lose more power in the server's power supply, stepping down the voltage to levels that are useful inside a computer, which is often about another 10% loss (not pictured in the figure below).</p> <p><img src=/images/datacenter-power/power_conversion.png alt="Power conversion figure from Hennessy & Patterson, which reproduced the figure from Hamilton"width=640 height=383></p> <p>And then once we get the power into servers, it gets turned into waste heat. To keep the servers from melting, we have to pay for power to cool them. <a href=http://www.morganclaypool.com/doi/abs/10.2200/S00193ED1V01Y200905CAC006>Barroso and Holzle</a> estimated that 30%-50% of the power drawn by a datacenter is used for chillers, and that an additional 10%-20% is for the <a href=http://www.dchuddle.com/2011/crac-v-crah/ >CRAC</a> (air circulation). That means for every watt of power used in the server, we pay for another 1-2 watts of support power.</p> <p>And to actually get all this power, we have to pay for the infrastructure required to get the power into and throughout the datacenter. Hennessy &amp; Patterson estimate that of the $90M cost of an example datacenter (just the facilities -- not the servers), 82% is associated with power and cooling<sup class=footnote-ref id=fnref:E><a rel=footnote href=#fn:E>1</a></sup>. The servers in the datacenter are estimated to only cost $70M. It's not fair to compare those numbers directly since servers need to get replaced more often than datacenters; once you take into account the cost over the entire lifetime of the datacenter, the amortized cost of power and cooling comes out to be 33% of the total cost, when servers have a 3 year lifetime and infrastructure has a 10-15 year lifetime.</p> <p>If we look at all the costs, the breakdown is:</p> <style>table{border-collapse:collapse;margin:0 auto}table,td,th{border:1px solid #000}td{text-align:center}td.l{text-align:left}</style> <table> <tr> <th>category<th>%</tr> <tr> <td>server machines<td>53</tr> <tr> <td>power &amp; cooling infra<td>20</tr> <tr> <td>power use<td>13</tr> <tr> <td>networking<td>8</tr> <tr> <td>other infra<td>4</tr> <tr> <td>humans<td>2</tr> </table> <p>Power use and people are the cost of operating the datacenter (OPEX), whereas server machines, networking, power &amp; cooling infra, and other infra are capital expenditures that are amortized across the lifetime of the datacenter (CAPEX).</p> <p>Computation uses a lot of power. <a href=http://www.datacenterknowledge.com/archives/2010/05/19/microsoft-building-new-data-center-in-quincy/ >We used to build steel mills near cheap sources of power</a>, but <a href=http://www.datacenterknowledge.com/archives/2010/05/19/microsoft-building-new-data-center-in-quincy/ >now that's where we build datacenters</a>. As companies start considering the full cost of applications, we're seeing a lot more power optimized solutions<sup class=footnote-ref id=fnref:1><a rel=footnote href=#fn:1>2</a></sup>. Unfortunately, this is really hard. On the software side, with the exceptions of toy microbenchmark examples, <a href=http://arcade.cs.columbia.edu/energy-oopsla14.pdf>best practices for writing power efficient code still aren't well understood</a>. On the hardware side, Intel recently released a new generation of chips with significantly improved performance per watt that doesn't have much better absolute performance than the previous generation. On the hardware accelerator front, some large companies are building dedicated power-efficient hardware for specific computations. But with existing tools, hardware accelerators are costly enough that dedicated hardware only makes sense for the largest companies. There isn't an easy answer to this problem.</p> <p><small> If you liked this post, you'd probably like chapter 6 of <a href="http://www.amazon.com/gp/product/012383872X/ref=as_li_qf_sp_asin_il_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=012383872X&amp;linkCode=as2&amp;tag=abroaview-20&amp;linkId=Y6Z2OBCUCR72ALEB">Hennessy &amp; Patterson</a>, which walks through not only the cost of power, but a number of related back of the envelope calculations relating to datacenter performance and cost.</small></p> <p>Apologies for the quickly scribbled down post. I jotted this down shortly before signing an NDA for an interview where I expected to learn some related information and I wanted to make sure I had my thoughts written down before there was any possibility of being contaminated with information that's under NDA.</p> <p>Thanks to Justin Blank for comments/corrections/discussion. </p> <div class=footnotes> <hr> <ol> <li id=fn:E><p>Although this figure is widely cited, I'm unsure about the original source. This is probably the most suspicious figure in this entire post. Hennessy &amp; Patterson cite “Hamilton 2010”, which appears to be a reference to <a href=http://mvdirona.com/jrh/TalksAndPapers/JamesHamilton_GenomicsCloud20100608.pdf>this presentation</a>. That presentation doesn't make the source of the number obvious, although <a href=http://perspectives.mvdirona.com/2008/11/cost-of-power-in-large-scale-data-centers/ >this post by Hamilton does cite a reference for that figure</a>, but the citation points to <a href=http://blogs.msdn.com/b/the_power_of_software/archive/2008/09/19/intense-computing-or-in-tents-computing.aspx>this post</a>, which seems to be about putting datacenters in tents, not the fraction of infrastructure that's dedicated to power and cooling.</p> <p>Some other works, <a href=http://science.energy.gov/~/media/ascr/ascac/pdf/meetings/mar09/Reed.pdf>such as this one</a> cite <a href=http://www.electronics-cooling.com/2007/02/in-the-data-center-power-and-cooling-costs-more-than-the-it-equipment-it-supports/ >this article</a>. However, that article doesn't directly state 82% anywhere, and it makes a number of estimates that the authors acknowledge are very rough, with qualifiers like “While, admittedly, the authors state that there is a large error band around this equation, it is very useful in capturing the magnitude of infrastructure cost.”</p> <a class=footnote-return href=#fnref:E><sup>[return]</sup></a></li> <li id=fn:1>That being said, power isn't everything -- Reddi et al. looked at replacing conventional chips with low-power chips for a real workload (MS Bing) and found that while they got an improvement in power use per query, tail latency increased significantly, especially when servers were heavily loaded. Since Bing has a mechanism that causes query-related computations to terminate early if latency thresholds are hit, the result is both higher latency and degraded search quality. <a class=footnote-return href=#fnref:1><sup>[return]</sup></a></li> </ol> </div> </main><nav><div class=np> <a href=monorepo/ >← Advantages of monorepos</a> <a href=dunning-kruger/ >Reading citations is easier than most people think →</a> </div> <div class=np> <a href= >Archive</a> <a href=https://www.patreon.com/danluu>Patreon</a> <a href=https://mastodon.social/@danluu>Mastodon</a> <a href=https://threads.net/@danluu.danluu>Threads</a> <a href=https://www.linkedin.com/in/danluu/ >LinkedIn</a> <a href=https://twitter.com/danluu/ >Twitter</a> </div></nav>